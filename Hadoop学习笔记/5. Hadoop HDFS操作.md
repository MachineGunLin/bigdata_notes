# 5. Hadoop HDFS操作

#### 启动HDFS

首先，格式化配置HDFS文件系统，打开NameNode（HDFS服务器），然后执行以下命令：

> $ hadoop namenode -format

![image-20200615115851890](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615115851.png)

格式化HDFS后，启动分布式文件系统。以下命令将启动名称节点namenode和数据节点datanode的集群。**注意要在hadoop目录下执行命令**：

> $ sbin/start-dfs.sh

![image-20200615120149031](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615120149.png)

#### HDFS的文件列表

加载服务器信息后，使用`ls`可以找出文件列表中的目录、文件状态。

下面是ls, 可以传递一个目录或文件名作为参数的语法：

> $ $HADOOP_HOME/bin/hadoop fs - ls <args>

#### 将数据插入到HDFS

假设在本地系统的file.txt文件中的数据应当保存在HDFS文件系统。

按照下面给出插入在Hadoop的文件系统所需要的文件的步骤。

第一步，必须创建一个输入目录。

> $ $HADOOP_HOME/bin/hadoop fs -mkdir /user/input![image-20200615124556221](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615124556.png)

第二步，传输并使用本地系统put命令，Hadoop文件系统中存储数据文件。

> $ $HADOOP_HOME/bin/hadoop fs -put /root/file.txt /user/input

![image-20200615124755971](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615124756.png)

第三步，使用ls命令验证文件。

> $ $HADOOP_HOME/bin/hadoop fs -ls /user/input

![image-20200615124904781](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615124904.png)

#### 从HDFS中检索数据

假设在HDFS中文件名为file.txt。

检索从Hadoop文件系统的文件：

第一步，使用cat命令查看来自HDFS的数据

> $ $HADOOP_HOME/bin/hadoop fs -cat /user/input/file.txt

![image-20200615125109030](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615125109.png)

第二步，从HDFS得到文件使用get命令到本地文件系统

> $ $HADOOP_HOME/bin/hadoop      fs    -get /user/input/     /root/hadoop_tp/

![image-20200615125641497](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615125641.png)

HDFS的文件get到了本地文件系统。

#### 关闭HDFS

> $ $HADOOP_HOME/sbin/stop-dfs.sh

![image-20200615125824698](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200615134150.png)