# 17. Spark reduceByKey函数

在spark中，`reduceByKey`函数是一种常用的转换操作，它执行数据聚合（聚合的规则自己定义）。

它接收键值对(K, V)作为输入，基于键聚合值并生成(K, V）对的数据集作为输出。

#### reduceByKey函数示例

在此示例中，我们基于键聚合值.

使用并行化集合创建RDD：

> scala> val data = sc.parallelize(Array(("C", 3), ("A", 1), ("B", 4), ("A", 2), ("B", 5)))

读取生成的结果：

> scala> data.collect

![image-20200612105256248](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111753.png)

应用`reduceByKey()`函数来聚合值。

> scala> val reducefunc = data.reduceByKey((value, x) => (value + x))

读取生成的结果：

> scala> reducefunc.collect

![image-20200612105529453](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612111805.png)

这里聚合的规则是把键相同的值都加起来，返回结果是键值对，每个键的值是原来数据集中这个键的所有值的总和。