# 21. Spark单词统计示例（word count）

在Spark单词统计示例中，将找出指定文件中存在的每个单词的出现频率。这里我们使用Scala语言来执行Spark操作。

#### 执行Spark字数计算示例的步骤

在此示例中，查找并显示每个单词的出现次数。在本地服务器中创建一个文本并在其中写入一些单词。

![image-20200612114032274](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114032.png)

在hadoop的目录中创建一个目录，保存文本文件：

> $ hdfs dfs -mkdir /spark

![image-20200612114904720](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114904.png)

将sparkdata.txt文件上传到特定目录中：

> $ hdfs dfs -put /root/sparkdata.txt /spark

![image-20200612115106288](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115106.png)

在Scala模式下打开Spark:

> $ spark-shell

![image-20200612115211199](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115211.png)

创建一个RDD：

> scala> val data = sc.textFile("../../sparkdata.txt")

这里传递包含数据的任何文件名。

读取生成的结果：

> scala> data.collect

![image-20200612115806248](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115806.png)

以单个单词的形式拆分现有数据（制表符作为分隔符）：

> scala> val splitdata = data.flatMap(line => line.split("\t"))

读取生成的结果：

> scala> splitdata.collect

![image-20200612120037176](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612120037.png)

执行映射操作：

> scala> val mapdata =  splitdata.map(word => (word, 1))

这里每个单词分配值**1**。

读取生成的结果：

> scala> mapdata.collect

![image-20200612120241496](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122613.png)

每个单词都有了一个值1.

现在执行`reduce`操作：

> scala> val reducedata = mapdata.reduceByKey(\_+\_)

这里汇总了生成的数据。

读取生成的数据：

> scala> reducedata.collect

![image-20200612120428781](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122622.png)

返回结果就是每个单词和他们出现的次数。