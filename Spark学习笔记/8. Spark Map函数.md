# 8. Spark Map函数

在Spark中，Map通过函数传递源的每个元素，并形成新的分布式数据集。

#### Map函数示例

为每一个元素添加一个常量值。在Scala模式下打开Spark：

> $ spark-shell

![image-20200612091716714](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612091757.png)

使用并行化集合创建RDD：

> scala> val data = sc.parallelize(List(10, 20, 30))

读取生成的结果：

> scala> data.collect

![image-20200612091941156](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612091941.png)

应用map函数并传递所需的表达式：

> scala: val mapfunc = data.map(x => x + 10)

读取生成的结果：

> scala> mapfunc.collect

![image-20200612092122547](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612092122.png)

data集合中的每个元素的值都加了10.