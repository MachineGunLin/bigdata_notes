# 16. Spark groupByKey函数

在spark中，`groupByKey`函数是一种经常使用的转换操作，它执行数据的**混乱**。

它接收键值对(K, V)作为输入，基于键值对进行分组，***key相同的value被分到一组***。并生成（K, Iterable）对的数据集作为输出。

#### groupByKey函数示例

使用并行化集合创建RDD：

> scala> val data = sc.parallelize(Seq(("C", 3), ("A", 1), (" B", 4), ("A", 2), ("B", 5)))

读取生成的结果：

> scala> data.collect

![image-20200612104346925](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612104346.png)

应用`groupByKey()`函数对键值对进行分组：

> scala> val groupfunc = data.groupByKey()

读取生成的结果：

> scala> groupfunc.collect

![image-20200612104534462](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612104534.png)