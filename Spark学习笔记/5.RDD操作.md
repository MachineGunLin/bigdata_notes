# 5.RDD操作

RDD提供两种类型的操作： 

1. 转换
2. 行动

#### 1. 转换

在Spark中，转换的作用是从现有数据集创建新数据集。

转换是惰性的，它们仅在动作需要将结果返回到驱动程序时才计算。

常用的RDD转换：

- `map(func)` ： 它返回一个新的分布式数据集，该数据集是通过`func`传递源的每个元素而形成的。
- `filter(func)` ： 它返回一个新数据集，该数据集是通过选择函数`func`返回`true`的源元素而形成的。
- `flatMap(func)` ： 这里每个输入项可以映射到零个或多个输出项，因此函数`func`应该返回序列而不是单个项。
- `mapPartitions(func)` ： 类似于`map(func)`，但是它是在RDD的每个分区（块）上单独运行，因此当在类型T的RDD上运行时，`func`必须是`Iterator <T> => Iterator <U>`类型。
- `mapPartitionsWithIndex(func)` ： 类似于`mapPartitons(func)`，它为`func`提供了一个表示分区索引的整数值，因此当在类型T的RDD上运行时，`func`必须是类型`(Int, Iterator <T>)=> Iterator <U>`。
- `sample(withReplacement, fraction, seed)` ： 它使用给定的随机数生成器种子对数据的分数部分进行采样，`withReplacement`表示有或者没有替换。
- `union(otherDataset)` ： 它返回一个新数据集，其中包含源数据集和参数中元素的并集。
- `intersection(otherDataset)` ： 它返回一个新RDD，其中包含源数据集和参数中的集合元素的交集。
- `distinct([numPartitions]))` ： 它返回一个新数据集，其中包含源数据集的不同元素。
- `groupByKey([numPartitions])` ：当在`(K, V)`对的数据集上调用时，它返回`(K, Iterable)`对的数据集。
- `reduceByKey(func, [numPartitions])` ： 当调用`(K, V)`对的数据集时，返回`(K, V)`对的数据集，其中使用给定的`reduce`函数`func`聚合每个键的值，该函数类型必须是`(V, V)=>V`。
- `aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])`  ： 当调用`(K, V)`对的数据集时，返回`(K, V)`对的数据集，其中使用给定的组合函数和中性”零“值聚合每个键的值。
- `sortByKey([ascending], [numPartitions])` ： 它返回按键值升序或降序排序的键值对的数据集，如在布尔`ascending`参数中指定。
- `join(otherDataset, [numPartitions])` ： 当调用类型`(K, V)`和`(K, W)`的数据集时，返回`(K, (V, W))`对的数据集以及每个键的所有元素对。通过`leftOuterJoin`，`rightOuterJoin`和`fullOuterJoin`支持外连接。
- `cogroup(otherDataset, [numPartitions])` ： 当调用类型`(K, V)`和`(K, W)`的数据集时，返回`(K, (Iterable, Iterable))`元祖的数据集。此操作也成为`groupWith`。
- `cartersian(otherDataset)` ： 当调用类型为T和U的数据集时，返回`(T, U)`对的数据集（所有元素对）。
- `pipe(command, [envVars])` ： 通过shell命令管道RDD的每个分区，例如一个perl或bash脚本。
- `coalesce(numPartitions)` ： 它将RDD中的分区数减少到`numPartitions`。
- `repartition(numPartitions)` ： 它随机重新调整RDD中的数据，以创建更多或更少的分区，并在它们之间进行平衡。
- `repartitionAndSortWithinPartitions(partitioner)` ： 它根据给定的分区器对RDD进行重新分区，并在每个生成的分区中的键值对记录进行排序。

#### 2. 操作

在Spark中，操作的作用是在对数据集运行计算后将值返回给驱动程序。

常用的RDD操作：

|                   操作                    |                             描述                             |
| :---------------------------------------: | :----------------------------------------------------------: |
|              `reduce(func)`               | 它使用函数func(它接受两个参数并返回一个)来聚合数据集的元素。该函数应该是可交换的和可关联的，以便可以并行正确计算。 |
|                `collect()`                | 它将数据集的所有元素作为数组返回到驱动程序中。在过滤器或其他返回足够小的数据子集的操作之后，这通常很有用。 |
|               `count(func)`               |                   返回数据集中的元素个数。                   |
|                 `first()`                 |         返回数据集的第一个元素（类似于`take(1)`）。          |
|                 `take(n)`                 |            返回一个包含数据集的前n个元素的数组。             |
| `takeSample(withReplcament, num, [seed])` | 返回一个数组，其中包含数据集的num个元素的随机样本，withReplacement表示有或者没有替换，seed是可选的，可以预先指定随机数生成器种子。 |
|       `takeOrdered(n, [ordering])`        |  它使用自然顺序或自定义比较器[ordering]返回RDD的前n个元素。  |
|          `saveAsTextFile(path)`           | 用于将数据集的元素作为文本文件（或文本文件集）写入本地文件系统，HDFS或任何其他Hadoop支持的文件系统的给定目录中。 |
|        `saveAsSequenceFile(path)`         | 它用于在本地文件系统，HDFS或任何其他Hadoop支持的文件系统中的给定路径中将数据集的元素编写为Hadoop SequenceFile。 |
|         `saveAsObjectFile(path)`          | 它用于使用java序列化以简单格式编写数据集的元素，然后可以使用`SparkContext.objectFile()`加载。 |
|              `countByKey()`               | 它仅适用于类型(K, V)的RDD。因此它返回（K, Int）对的散列映射与每个键的计数。 |
|              `foreach(func)`              | 它在数据集的每个元素上运行函数`func`以获得副作用，例如更新累加器或与外部存储系统交互。 |

