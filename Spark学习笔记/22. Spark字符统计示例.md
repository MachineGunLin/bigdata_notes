# 22. Spark字符统计示例

在Spark字符统计示例中，将找出指定文件中每个字符的频率。这里使用scala语言来执行spark操作。

#### 执行Spark字符计数示例的步骤

写好一个sparkdata.txt:

![image-20200612120827865](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122541.png)

在hadoop的目录中创建一个目录，保存文本文件：

> $ hdfs dfs -mkdir /spark

![image-20200612114904720](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612114904.png)

将sparkdata.txt文件上传到特定目录中：

> $ hdfs dfs -put /root/sparkdata.txt /spark

![image-20200612115106288](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115106.png)

在scala模式下打开spark:

> $ spark-shell

![image-20200612120933126](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122530.png)

创建一个RDD：

> scala> val data = sc.textFile("../../sparkdata.txt")

这里传递包含数据的文件名。

读取生成的结果：

> scala> data.collect

![image-20200612115806248](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612115806.png)

以单个字符的形式拆分现有数据：

> scala> val splitdata  = data.flatMap(line => line.split(""))

读取生成的结果：

> scala> splitdata.collect

![image-20200612121913070](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612121913.png)

执行映射操作：

> scala> val mapdata = splitdata.map(word => (word, 1))

这里给每个字符分配了一个值**1**。

读取生成的结果：

> scala> mapdata.collect

![image-20200612122052952](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122053.png)

执行`reduce`操作：

> scala> val reducedata = mapdata.reduceByKey(\_+\_)

读取生成的结果：

> scala> reducedata.collect

![image-20200612122456444](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200612122456.png)

返回结果是计算所有字符及它们的出现次数。